{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, you will learn how to construct a classification algorithm for determining whether a device is operating on textile carpet or hard floor. The information of the surface type can be used in various applications, such as manual and robotic vacuum cleaners for adjusting the operation of the unit for optimal performance.\n",
    "\n",
    "Follow along the steps outline below, detailing data collection, model definition, training and evaluation. Once the model is ready, you will be able to run it on your computer and classify surfaces in realtime. \n",
    "\n",
    "If you want to learn more about Acconeer's sensors, algorithms and more, please visit the documentation and developer site, found [here](https://docs.acconeer.com/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test setup\n",
    "\n",
    "The required hardware for this example is\n",
    "* XE121 - A121 evaluation kit. \n",
    "* XS121 - A121 satellite senors.\n",
    "* Raspberry Pi.\n",
    "* A powerbank.\n",
    "* The intended device of operation.\n",
    "\n",
    "The hardware from Acconeer can be bought at a number of online vendors such as [Digikey](https://www.digikey.com/) or [Mouser](https://www.mouser.se/c/unclassified/acconeer-ab/?q=acconeer).\n",
    "\n",
    "To get your EVK up and running with the satellite sensor and Raspberry, visit our documentation and developer site, found [here](https://docs.acconeer.com/en/latest/index.html).\n",
    "\n",
    "We will use a Kobuki as our device in this example. \n",
    "Kobuki is an all purpose robotic device, used for rapid prototyping.\n",
    "For more information about Kobuki, follow this [link](http://kobuki.yujinrobot.com/about2/). \n",
    "\n",
    "The following picture gives an overview of the setup. \n",
    "* The XS121 is mounted at the front of the Kobuki using a 3D-printed sensor holder. If you don't have access to a 3D-printer, some cardboard and tape would work just fine.\n",
    "* The sensor holder is angled at 40 deg relative to the normal of the floor. \n",
    "* The flex cable connects the sensor to the EVK board.\n",
    "* The XE121 is connected to a Raspberry Pi, running the Acconeer's exploration server.\n",
    "* The Raspberry Pi is powered by a power bank.\n",
    "\n",
    "![Setup](doc/setup.png)\n",
    "\n",
    "The purpose of tilting the angle of the sensor is to get sufficient energy back from both the floor directly under the sensor, but also from areas in front of the sensor.\n",
    "The reason why this is necessary will become apparent when the features of the classification algorithm are discussed in a coming section.\n",
    "For more information about the beam pattern and HPBW(Half Power Beam Width), see the documentation site.\n",
    "\n",
    "To run this example, you also need to install Acconeer's Exploration Tool. \n",
    "The GUI part of the tool will be used during data collection. Several functions from the package are used in this example for loading and processing the data.\n",
    "We encourage you to play around with the different examples and applications in the Exploration Tool to get familiar with the sensor.\n",
    "\n",
    "For more information on how to install Exploration Tool and get started with your hardware, see the documentation site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface types\n",
    "Three different hard floors and three carpets were used in this example.\n",
    "\n",
    "The following picture shows the different surface types\n",
    "\n",
    "![Surface types](doc\\surface_types.png)\n",
    "\n",
    "The characteristics of the surface is as follows(from left to right):\n",
    "* Large tiles.\n",
    "* Wood floor.\n",
    "* Plastic/lenolium floor.\n",
    "* Low fiber density carpet.\n",
    "* High fiber density carpet.\n",
    "* Thin carpet.\n",
    "\n",
    "Once you have completed this example, we encourage you to record your own data from available surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features\n",
    "\n",
    "The following two features will be extracted from the radar data and used when classifying the surface. The calculations are implemented in the included module *utils.py* in the functions *estimate_distance* and *calculate_variance_at_fixed_distance*.\n",
    "\n",
    "### #1 - Distance to floor\n",
    "\n",
    "The concept behind this feature is that when the device is running directly on a hard floor, the estimated distance will be close to the nominal height at which the sensor has been installed, relative to the floor. \n",
    "Next, when the device moves onto a textile carpet, the underlying carpet will contribute to an increased distance between the sensor and the floor. \n",
    "As textile carpets typically are relative transparent, the majority of the reflected energy will come from the underlying floor, and it therefor still possible to estimate the distance to the floor, which is now greater as the device is operating at a greater distance from the underlying floor.\n",
    "\n",
    "The following picture illustrates the amplitude of the measured signal vs distance from the sensor in the two cases.\n",
    "The distance to the floor is estimated as the location of the peak amplitude. \n",
    "As the sensor has a fairly wide HPBW, the majority of the reflected energy will come from the area directly underneath the sensor, yielding a large amplitude at the corresponding distance.\n",
    "Before the location of the amplitude is determined, the signal is smoothened using a second order Butterworth signal to get a more consistent estimate.\n",
    "\n",
    "![Distance estimation](\\doc\\distance_to_floor_plot.png)\n",
    "\n",
    "For details, see *estimate_distance* in *utils.py*.\n",
    "\n",
    "### #2 - Variance of data\n",
    "\n",
    "The purpose of this feature is to capture the amount of reflected energy at a given distance from the sensor, where the hypothesis is that an uneven surface such as a carpet will reflect more energy back towards the sensor, while in the case of a smooth floor, the majority of the energy bounces of the surface and continue away from the sensor.\n",
    "\n",
    "The plot below shows the data points at a given distance over multiple sweeps in the complex plane. \n",
    "If you are not yet familiar with the data produced by the a121 sensor, see the section about sparse IQ on the [docmentation site](https://docs.acconeer.com/en/latest/handbook/a121/interpreting_radar_data.html). \n",
    "As can be seen, the amplitude of the signals is greater for the carpet compared to the floor, indicating more energy being reflected back towards the sensor. \n",
    "The reason for using the variance as a metric instead of the absolute values is to alleviate the issue with strong static direct leakage when measuring close to the sensor. \n",
    "Direct leakage is the energy traveling directly from the transmitting to receiving antenna, and is present when measuring close to the sensor, resulting in a strong static component in the signal.\n",
    "\n",
    "![Signal variance](\\doc\\signal_variance.png)\n",
    "\n",
    "For details, see *calculate_variance_at_fixed_distance* in *utils.py*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor settings\n",
    "\n",
    "The used sensor configuration is motivated and discussed in this section.\n",
    "For more information on what the different configuration parameters do and recommended settings for various use cases, see the [documentation site](https://docs.acconeer.com/en/latest/index.html).\n",
    "\n",
    "**Profile** \n",
    "The profile determines the length of the transmitted pulse. \n",
    "As we will be measuring close to the sensor, it is recommended to use a profile that corresponds to a shorter pulse to minimize the impact of direct leakage(energy traveling straight from the transmitting to the receiving antenna).\n",
    "\n",
    "**Start point**\n",
    "The start point is set to 24, which corresponds to 60mm.\n",
    "This value is selected as the sensor is mounted roughly 90mm from the floor. \n",
    "If the sensor is mounted closer to the floor, the measured reflected energy will start to interact with the direct leakage.\n",
    "In this case, some method for handling this interaction needs to be applied.\n",
    "One approach is to use the close range measurement strategy, employed by for instance the distance detector.\n",
    "For more information, see the [distance detector documentation](https://docs.acconeer.com/en/latest/exploration_tool/algo/a121/distance_detection.html).\n",
    "\n",
    "**Step length**\n",
    "The step length is set to 1 as we require high resolution in the distance domain.\n",
    "\n",
    "**Num points**\n",
    "The number of points is set to 40, corresponding to 100mm, yielding a total measurement interval of [60, 160]mm.\n",
    "\n",
    "To view the full set of sensor setting, load one of the included data files into the Sparse IQ service in the Exploration Tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification algorithm overview\n",
    "The classification algorithm used in this example is a small neural network, implemented through Google's Tensorflow/Keras framework. Feel free to play around with other classifiers and see if you can achieve better performance.\n",
    "\n",
    "The network has three densely connected layers, around 150 trainable parameters and is sufficiently small to be implemented in an embedded environment.\n",
    "\n",
    "The last layer of the model has been configured with a softmax activation function, yielding a output vector that reflects the probability of each class. \n",
    "\n",
    "After the model has been trained and validated against test data, it is saved, to later be loaded into realtime_deployment.py script, running the model in realtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording data\n",
    "\n",
    "The data is recorded using the Exploration Tool.\n",
    "To retrieve the recommended sensor settings for this example, load one of the provided data files.\n",
    "\n",
    "For each surface type, move or navigate your device over the surface while recording data. \n",
    "Once done, stop and save the data file with the following file format:\n",
    "\n",
    "*surface_type_index*\n",
    "\n",
    "where *surface_type* is either *floor* or *surface* and *index* is just an interger, used to distinguish multiple data files with data from the same class.\n",
    "Following this file name standard is important for the remainder of this example as the class label is extracted from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To the code\n",
    "The first step is to import all the necessary packages. \n",
    "If you are missing any of them, please install and then proceed.\n",
    "\n",
    "The *utils.py*-module contains functions that are used both in this notebook, as well as in the *realtime_deployment.py*-script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from acconeer.exptool.a121 import load_record\n",
    "from acconeer.exptool.a121.algo import interpolate_peaks, APPROX_BASE_STEP_LENGTH_M\n",
    "\n",
    "from utils import estimate_distance, calculate_variance_at_fixed_distance, plot_feature_by_file_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load all the data files in the *data*-folder.\n",
    "Note, as previously mentioned, it is important to follow the specified naming convention of the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('data/*.h5')\n",
    "print('Loaded files:', filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet handles the features and class label extraction \n",
    "The result is saved in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "file_num = []\n",
    "feature_variance = []\n",
    "feature_distance_to_floor = [] \n",
    "for filename in filenames:\n",
    "    record = load_record(filename)\n",
    "    sensor_config = record.session_config.sensor_config\n",
    "    frames = record.frames\n",
    "\n",
    "    num_features_extracted = 0\n",
    "    for frame in frames:\n",
    "        # extract peak location\n",
    "        distance_to_floor = estimate_distance(frame, sensor_config)\n",
    "        if len(distance_to_floor) == 0:\n",
    "            # No peak found. Proceed to next frame. \n",
    "            continue \n",
    "        feature_distance_to_floor.append(distance_to_floor[0])\n",
    "        \n",
    "        # extract variance data\n",
    "        feature_variance.append(calculate_variance_at_fixed_distance(frame))\n",
    "\n",
    "        num_features_extracted += 1\n",
    "\n",
    "    # extract class information from file name\n",
    "    filename_info = filename.replace('data\\\\',\"\").replace('.h5',\"\").split('_')\n",
    "    labels = labels + [filename_info[0]] * num_features_extracted\n",
    "    file_num = file_num + [int(filename_info[1])] * num_features_extracted\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['labels'] = labels\n",
    "df['file_num'] = file_num\n",
    "df['feature_variance'] = feature_variance\n",
    "df['feature_distance_to_floor'] = feature_distance_to_floor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we inspect the features for our two classes(floor and carpet).\n",
    "\n",
    "The first feature is the distance to the floor. As can be seen in the two graphs below, the distribution when running on the carpet located more to the right, representing the greater distance between the floor and the sensor. \n",
    "The separation between the distributions of the two different classes is fairly distinct, indicating that this is a promising feature for distinguishing the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_by_file_number(df, 'feature_distance_to_floor', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the second feature - the variance of the reflected energy.\n",
    "\n",
    "As previously discussed, this feature characterizes the amount of reflected energy back towards the sensor.\n",
    "The hypothesis is that the carpets more uneven surface will cause more energy to be reflected back to the sensor, compared to a more smooth surface, such as a hard floor.\n",
    "\n",
    "As can be seen in the two histograms below, this seems to be just the case - the carpet has a wider distribution and greater values, while the floor has a narrower distribution with smaller values.\n",
    "\n",
    "The distributions are somewhat overlapping.\n",
    "Hence, this feature on its own could cause a greater rate of false positives.\n",
    "However, as will be shown in a later segment, combining the two features yields a high classification accuracy rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_by_file_number(df, 'feature_variance', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our features are extracted, prepared and examined, it is time to move over to the model training part.\n",
    "The first step is to divide the data into a training and a test set. We you a 70/30 split for the training and the test set.\n",
    "Before splitting the set, the labels are onehot encoded, representing the sting labels with integers, allowing for numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['feature_distance_to_floor', 'feature_variance']\n",
    "\n",
    "X = np.array(df[features], dtype='float32')\n",
    "y = np.array(df['labels']).reshape(-1,1)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "y = enc.transform(y).toarray()\n",
    "print('Classes : ',enc.categories_)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we defined the classification model. \n",
    "\n",
    "As previously mentioned, a neural network with three densely connected layers is selected.\n",
    "The reason for selecting a relatively simple model(few layers with a modest number of nodes) is that the we only have two features and their distributions are fairly well separated for the two classes.\n",
    "\n",
    "We also use some standard techniques such as data normalization and dropout to improve the performance and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and compile model\n",
    "normalization_layer = layers.Normalization(axis=-1)\n",
    "normalization_layer.adapt(X)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(len(features)))\n",
    "\n",
    "x = normalization_layer(inputs)\n",
    "x = layers.Dense(units=10, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(units=5, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(units=y.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',    \n",
    "    metrics=['accuracy'], \n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we train the model and print the result for each epoch.\n",
    "\n",
    "We usually achieve results in the range of 96-98% accuracy on the test set.\n",
    "Feel free to modify the model and see if you can achieve better results.\n",
    "Keras offers a wide variety of models and layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=10,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the training is plotted vs each epoch to illustrate the progression of the metrics between each iteration.\n",
    "\n",
    "As can be seen, the model achieves a low cost function value in just a few iterations, indicating that the model is fitted to the data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "history_df = history.history\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.plot(history_df['loss'],label='Loss function')\n",
    "plt.plot(history_df['accuracy'],label='Prediction accuracy')\n",
    "plt.plot(history_df['val_loss'],label='Validation data - Loss function')\n",
    "plt.plot(history_df['val_accuracy'],label='Validation data - Prediction accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below calculates and prints the confusion matrix. \n",
    "\n",
    "As can be seen, the false positive rate is high and the false positive rate is close to equally distributed between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print('Classes : ',enc.categories_)\n",
    "confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the model is saved. After running this code, a folder named *model* will show up in you working directory.\n",
    "This folder contains the trained model an meta data used by the script *realtime_deployment.py*.\n",
    "Note, if you would like to run this example in realtime on you own, it is very important that the sensor installation closely matches the one we used when collecting data.\n",
    "If this is not possible or practical, we recommend you to run data collection with your own sensor installation and then rerun this notebook to produce a model that represents your sensor installation and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a5913bd725b557028b0849e16e8d5d9f54d1e31c0a82d309e3b2aa345c5900a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
